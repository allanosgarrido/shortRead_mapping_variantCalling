import os
import glob
import re
from collections import defaultdict

configfile: "config.yaml"
CLUSTER = json.load(open(config['CLUSTER_JSON']))

# rename variables from config file for clarity downstream
fastq_suffix1 = config["fastq_suffix1"]
fastq_suffix2 = config["fastq_suffix2"]
fastqDir = config["fastqDir"]
bamDir = config["bamDir"]
sumstatDir = config["sumstatDir"]

# grab all samples for R1 to get list of names, no need to look at R2 which should have identical names
SAMPLES = glob.glob(fastqDir + "*" + fastq_suffix1)	

print(fastq_suffix1)
print(fastq_suffix2)
print(fastqDir)
print(bamDir)
print(SAMPLES)

print("SAMPLES:")
for i in range(len(SAMPLES)):
    SAMPLES[i] = os.path.basename(SAMPLES[i])
    SAMPLES[i] = SAMPLES[i].replace(fastq_suffix1, "")
    print(SAMPLES[i])


###
# workflow with rules
###

rule all:
    input:
        "sumstats.txt"
	
rule index_ref:
    input:
        ref = config['ref']
    output: 
       config['ref'] + ".sa",
       config['ref'] + ".pac",
       config['ref'] + ".bwt",
       config['ref'] + ".ann",
       config['ref'] + ".amb"
    shell:
        "bwa index {input.ref}"

rule bwa_map:
    input:
        ref = config['ref'],
        r1 = fastqDir + "{sample}" + fastq_suffix1,
        r2 = fastqDir + "{sample}" + fastq_suffix2,
        # the following files are bwa index files that aren't directly input into command below, but needed
        sa = config['ref'] + ".sa",
        pac = config['ref'] + ".pac",
        bwt = config['ref'] + ".bwt",
        ann = config['ref'] + ".ann",
        amb = config['ref'] + ".amb"
    output: 
        bamDir + "{sample}.bam"
    threads: 
        CLUSTER["bwa_map"]["n"]
    params:
        rg="@RG\\tID:{sample}\\tSM:{sample}\\tPL:ILLUMINA"
    shell:
        "bwa mem -M -t {threads} -R \'{params.rg}\' {input.ref} {input.r1} {input.r2} | "
        "/n/home11/bjarnold/programs/samtools-1.10/samtools view -Sb - > {output}"

rule sort_bam:
    input: 
        bamDir + "{sample}.bam"
    output: 
        temp(bamDir + "{sample}_sorted.bam"),
        temp(bamDir + "{sample}_sorted.bai")
    run:
        shell("java -jar /n/home11/bjarnold/picard.jar SortSam TMP_DIR={bamDir}tmp I={input} O={output[0]} SORT_ORDER=coordinate CREATE_INDEX=true")
        shell("rm {input}")

rule dedup:
    input: 
        bamDir + "{sample}_sorted.bam",
        bamDir + "{sample}_sorted.bai"
    output:
        dedupBam = temp(bamDir + "{sample}_dedup.bam"),
        dedupMet = sumstatDir + "{sample}_dedupMetrics.txt",
        dedupBamSort = bamDir + "{sample}_dedupSort.bam"
    run:
        shell("java -jar /n/home11/bjarnold/picard.jar MarkDuplicates TMP_DIR={bamDir}tmp I={input[0]} O={output.dedupBam} METRICS_FILE={output.dedupMet} REMOVE_DUPLICATES=false TAGGING_POLICY=All")
        shell("java -jar /n/home11/bjarnold/picard.jar SortSam TMP_DIR={bamDir}tmp I={output.dedupBam} O={output.dedupBamSort} SORT_ORDER=coordinate CREATE_INDEX=true")

rule bam_sumstats:
    input: 
        bam = bamDir + "{sample}_dedupSort.bam",
        ref = config['ref']

    output: 
        cov = sumstatDir + "{sample}_coverage.txt",
        alnSum = sumstatDir + "{sample}_AlnSumMets.txt",
        val = sumstatDir + "{sample}_validate.txt"
    run:
        shell("/n/home11/bjarnold/programs/samtools-1.10/samtools coverage --output {output.cov} {input.bam}")
        shell("java -jar /n/home11/bjarnold/picard.jar CollectAlignmentSummaryMetrics I={input.bam} R={input.ref} O={output.alnSum}")
        # The following ValidateSamFile exits with non-zero status when a BAM file contains errors, 
        # causing snakemake to exit and remove these output files.  I cirumvent this by appending "|| true".
        shell("java -jar /n/home11/bjarnold/picard.jar ValidateSamFile I={input.bam} R={input.ref} O={output.val} || true")
		
rule collect_sumstats:
    input:
        dedupFiles = expand(sumstatDir + "{sample}_dedupMetrics.txt", sample=SAMPLES),
        alnSumMetsFiles = expand(sumstatDir + "{sample}_AlnSumMets.txt", sample=SAMPLES),
        coverageFiles = expand(sumstatDir + "{sample}_coverage.txt", sample=SAMPLES),
        validateFiles = expand(sumstatDir + "{sample}_validate.txt", sample=SAMPLES)
    output:
        "sumstats.txt"
    run:
        PercentDuplicates = collectDedupMetrics(input.dedupFiles)
        PercentHQreads, PercentHQbases = collectAlnSumMets(input.alnSumMetsFiles)
        SeqDepths, CoveredBases = collectCoverageMetrics(input.coverageFiles)
        validateSams = collectValidationStatus(input.validateFiles)

        printStats(PercentDuplicates, PercentHQreads, PercentHQbases, SeqDepths, CoveredBases, validateSams)

####
# python helper functions
###

def collectDedupMetrics(dedupFiles):

    PercentDuplicates = defaultdict(float)
    for fn in dedupFiles:
        sample = os.path.basename(fn)
        sample = sample.replace("_dedupMetrics.txt", "")
        f = open(fn, 'r')
        lines = f.readlines()
        for i in range(len(lines)):
            if lines[i].startswith("LIBRARY"):
                info = lines[i+1]
                info = info.split("\t")
                percDup = info[8]
                #print(percDup)
                PercentDuplicates[sample] = percDup
        f.close()
    return(PercentDuplicates)


def collectAlnSumMets(alnSumMetsFiles):

    PercentHQreads = defaultdict(float)
    PercentHQbases = defaultdict(float)

    for fn in alnSumMetsFiles:
        sample = os.path.basename(fn)
        sample = sample.replace("_AlnSumMets.txt", "")
        f = open(fn, 'r')
        for line in f:
            if line.startswith("PAIR"):
                line = line.split()
                totalReads = int(line[1])
                totalAlignedBases = int(line[7])
                HQreads = int(line[8])
                totalAlignedHQ20bases = int(line[10])
                PercentHQreads[sample] = HQreads/totalReads
                PercentHQbases[sample] = totalAlignedHQ20bases/totalAlignedBases
        f.close()
    return(PercentHQreads, PercentHQbases)

def collectCoverageMetrics(coverageFiles):

    SeqDepths = defaultdict(float)
    CoveredBases = defaultdict(float)

    for fn in coverageFiles:
        # these files contain coverage data by scaffold; take weighted average
        sample = os.path.basename(fn)
        sample = sample.replace("_coverage.txt", "")
        numSites = []
        covbases = 0
        depths = [] # samtools coverage function prints depth per scaffold
        f = open(fn, 'r')
        for line in f:
            if not line.startswith("#rname"):
                line = line.split()
                numSites.append( int(line[2]) - int(line[1]) + 1 )
                depths.append( float(line[6]) )
                covbases +=  float(line[4])
        f.close()
        total = sum(numSites)
        depthsMean = 0
        for i in range(len(depths)):
            depthsMean += depths[i]*numSites[i]/(total)
        SeqDepths[sample] = depthsMean
        CoveredBases[sample] = covbases
    return(SeqDepths, CoveredBases)

def collectValidationStatus(validateFiles):

    validateSams = defaultdict(float)

    for fn in validateFiles:
        sample = os.path.basename(fn)
        sample = sample.replace("_validate.txt", "")
        f = open(fn, 'r')
        status = "FALSE"
        for line in f:
            if "No errors found" in line:
                status = "TRUE"
        f.close()
        validateSams[sample] = status
    return(validateSams)

def printStats(PercentDuplicates, PercentHQreads, PercentHQbases, SeqDepths, CoveredBases, validateSams):

    o = open("sumstats.txt", 'w')
    print("sample", "PercentDuplicates", "PercentHQalignedReads", "PercentHQ20bases", "MeanCoverage", "MeanBasesCovered", "validBAM", file=o, sep="\t")
    for sample in PercentDuplicates:
        print(sample,file=o, end="\t")
        print(PercentDuplicates[sample], file=o, end="\t")
        print(PercentHQreads[sample], file=o, end="\t")
        print(PercentHQbases[sample], file=o, end="\t")
        print(SeqDepths[sample], file=o, end="\t")
        print(CoveredBases[sample], file=o, end="\t")
        print(validateSams[sample], file=o)


