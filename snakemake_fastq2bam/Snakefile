import os
import glob
import re
from collections import defaultdict

configfile: "config.yaml"
CLUSTER = json.load(open(config['CLUSTER_JSON']))

# rename variables from config file for clarity downstream
fastq_suffix1 = config["fastq_suffix1"]
fastq_suffix2 = config["fastq_suffix2"]
fastqDir = config["fastqDir"]
newBamDir = config["newBamDir"]
sumstatDir = config["sumstatDir"]

# grab all samples for R1 to get list of names, no need to look at R2 which should have identical names
SAMPLES = glob.glob(fastqDir + "*" + fastq_suffix1)	

print(fastq_suffix1)
print(fastq_suffix2)
print(fastqDir)
print(newBamDir)
print(SAMPLES)

print("SAMPLES:")
for i in range(len(SAMPLES)):
	SAMPLES[i] = os.path.basename(SAMPLES[i])
	SAMPLES[i] = SAMPLES[i].replace(fastq_suffix1, "")
	print(SAMPLES[i])


###
# workflow with rules
###

rule all:
	input:
		"sumstats.txt"
	
rule bwa_map:
	input:
		ref = config['ref'],
		r1 = fastqDir + "{sample}" + fastq_suffix1,
		r2 = fastqDir + "{sample}" + fastq_suffix2
	output: 
		newBamDir + "{sample}.bam"
	threads: 
		CLUSTER["bwa_map"]["n"]
	params:
		rg="@RG\\tID:{sample}\\tSM:{sample}\\tPL:ILLUMINA"
	shell:
		"bwa mem -M -t {threads} -R \'{params.rg}\' {input.ref} {input.r1} {input.r2} | "
		"/n/home11/bjarnold/programs/samtools-1.10/samtools view -Sb - > {output}"

rule sort_bam:
	input: 
		newBamDir + "{sample}.bam"
	output: 
		temp(newBamDir + "{sample}_sorted.bam"),
		temp(newBamDir + "{sample}_sorted.bai")
	# use run with multiple shell() calls to execute multiple shell commands
	run:
		shell("java -jar /n/home11/bjarnold/picard.jar SortSam TMP_DIR={newBamDir}tmp I={input} O={output[0]} SORT_ORDER=coordinate CREATE_INDEX=true")
		shell("rm {input}")

rule dedup:
	input: 
		newBamDir + "{sample}_sorted.bam",
		newBamDir + "{sample}_sorted.bai"
	output:
		dedupBam = temp(newBamDir + "{sample}_dedup.bam"),
		dedupMet = sumstatDir + "{sample}_dedupMetrics.txt",
		dedupBamSort = newBamDir + "{sample}_dedupSort.bam"
	run:
		shell("java -jar /n/home11/bjarnold/picard.jar MarkDuplicates TMP_DIR={newBamDir}tmp I={input[0]} O={output.dedupBam} METRICS_FILE={output.dedupMet} REMOVE_DUPLICATES=false TAGGING_POLICY=All")
		shell("java -jar /n/home11/bjarnold/picard.jar SortSam TMP_DIR={newBamDir}tmp I={output.dedupBam} O={output.dedupBamSort} SORT_ORDER=coordinate CREATE_INDEX=true")
		#shell("rm {input[0]} {input[1]}")

rule bam_sumstats:
	input: 
		newBamDir + "{sample}_dedupSort.bam",
		ref = config['ref']

	output: 
		sumstatDir + "{sample}_coverage.txt",
		sumstatDir + "{sample}_AlnSumMets.txt"
	run:
		shell("/n/home11/bjarnold/programs/samtools-1.10/samtools coverage --output {output[0]} {input[0]}")
		shell("java -jar /n/home11/bjarnold/picard.jar CollectAlignmentSummaryMetrics I={input[0]} R={input.ref} O={output[1]}")
		
rule collect_sumstats:
	input:
		dedupFiles = expand(sumstatDir + "{sample}_dedupMetrics.txt", sample=SAMPLES),
		alnSumMetsFiles = expand(sumstatDir + "{sample}_AlnSumMets.txt", sample=SAMPLES),
		coverageFiles = expand(sumstatDir + "{sample}_coverage.txt", sample=SAMPLES)	
	output:
		"sumstats.txt"
	run:
		PercentDuplicates = collectDedupMetrics(input.dedupFiles)
		PercentHQreads, PercentHQbases = collectAlnSumMets(input.alnSumMetsFiles)
		SeqDepths, CoveredBases = collectCoverageMetrics(input.coverageFiles)

		printStats(PercentDuplicates, PercentHQreads, PercentHQbases, SeqDepths, CoveredBases)

####
# python helper functions
###

def collectDedupMetrics(dedupFiles):

	PercentDuplicates = defaultdict(float)
	for fn in dedupFiles:
		sample = os.path.basename(fn)
		sample = sample.replace("_dedupMetrics.txt", "")
		f = open(fn, 'r')
		lines = f.readlines()
		for i in range(len(lines)):
			if lines[i].startswith("LIBRARY"):
				info = lines[i+1]
				info = info.split("\t")
				percDup = info[8]
				#print(percDup)
				PercentDuplicates[sample] = percDup
		f.close()
	return(PercentDuplicates)


def collectAlnSumMets(alnSumMetsFiles):

	PercentHQreads = defaultdict(float)
	PercentHQbases = defaultdict(float)

	for fn in alnSumMetsFiles:
		sample = os.path.basename(fn)
		sample = sample.replace("_AlnSumMets.txt", "")
		f = open(fn, 'r')
		for line in f:
			if line.startswith("PAIR"):
				line = line.split()
				totalReads = int(line[1])
				totalAlignedBases = int(line[7])
				HQreads = int(line[8])
				totalAlignedHQ20bases = int(line[10])
				PercentHQreads[sample] = HQreads/totalReads
				PercentHQbases[sample] = totalAlignedHQ20bases/totalAlignedBases
		f.close()
	return(PercentHQreads, PercentHQbases)

def collectCoverageMetrics(coverageFiles):

	SeqDepths = defaultdict(float)
	CoveredBases = defaultdict(float)

	for fn in coverageFiles:
		# these files contain coverage data by scaffold; take weighted average
		sample = os.path.basename(fn)
		sample = sample.replace("_coverage.txt", "")
		numSites = []
		covbases = 0
		depths = [] # samtools coverage function prints depth per scaffold
		f = open(fn, 'r')
		for line in f:
			if not line.startswith("#rname"):
				line = line.split()
				numSites.append( int(line[2]) - int(line[1]) + 1 )
				depths.append( float(line[6]) )
				covbases +=  float(line[4])
		f.close()
		total = sum(numSites)
		depthsMean = 0
		for i in range(len(depths)):
			depthsMean += depths[i]*numSites[i]/(total)
		SeqDepths[sample] = depthsMean
		CoveredBases[sample] = covbases
	return(SeqDepths, CoveredBases)

def printStats(PercentDuplicates, PercentHQreads, PercentHQbases, SeqDepths, CoveredBases):

	o = open("sumstats.txt", 'w')
	print("sample", "PercentDuplicates", "PercentHQalignedReads", "PercentHQ20bases", "MeanCoverage", "MeanBasesCovered", file=o, sep="\t")
	for sample in PercentDuplicates:
		print(sample, PercentDuplicates[sample], PercentHQreads[sample], PercentHQbases[sample], SeqDepths[sample], CoveredBases[sample], file=o, sep="\t")


